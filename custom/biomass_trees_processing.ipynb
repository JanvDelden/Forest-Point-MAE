{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os   \n",
    "import open3d as o3d\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass = np.loadtxt(\"../data/biomass/biomass.txt\", dtype=str)\n",
    "biomass = np.array([string.replace(\",\", \".\") for string in biomass])\n",
    "biomass = biomass.astype(float)\n",
    "path = \"../data/biomass/biomass.npy\"\n",
    "np.save(path, biomass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(biomass), bins=10)\n",
    "plt.savefig(fname=\"plot/biomass_hist\",format=\"svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass = np.log(biomass)\n",
    "mean_logmass = biomass.mean()\n",
    "std_logmass = biomass.std()\n",
    "biomass = (biomass - biomass.mean()) / biomass.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../data/biomass/trees\"\n",
    "samples_list = os.listdir(dir)\n",
    "sample_numbers = [re.findall(r'\\d+', sample)  for sample in samples_list]\n",
    "sample_numbers = [int(sample[0]) for sample in sample_numbers]\n",
    "sort_idx = np.array(sample_numbers).argsort()\n",
    "samples_list = [os.path.join(dir, sample) for sample in samples_list]\n",
    "samples_list = np.array(samples_list)[sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL=1000\n",
    "def voxelize(points, voxelsize=0.10):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    #print(f\"Voxelize the point cloud with a voxel size of {voxelsize}\")\n",
    "    min_bound, max_bound = np.array([-VAL, -VAL, -VAL]), np.array([VAL, VAL, VAL])\n",
    "    downpcd, _, idx = pcd.voxel_down_sample_and_trace(voxelsize, min_bound, max_bound)\n",
    "    print(f\"Previous size: {points.shape[0]}, new size {np.asarray(downpcd.points).shape[0]}\")\n",
    "    return np.array(downpcd.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"../data/trees/biomass-trees\"\n",
    "\n",
    "for idx, (mass, sample) in enumerate(zip(biomass, samples_list)):\n",
    "    path = os.path.join(savedir, str(idx) + \".npy\")\n",
    "    #if os.path.exists(path):\n",
    "    #    continue\n",
    "    pt = np.loadtxt(sample)\n",
    "    pt = voxelize(pt)\n",
    "    \n",
    "    np.save(path, (mass, pt))\n",
    "    #print(idx, \"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore simple methods for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss = torch.nn.MSELoss()\n",
    "prediction = biomass.mean()\n",
    "loss(torch.tensor(prediction), torch.tensor(biomass))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.parser as parser\n",
    "from utils.config import *\n",
    "import tools.builder as builder\n",
    "\n",
    "argv = [\"--config\", \"cfgs/regression/biomass_treeset.yaml\"]\n",
    "args = parser.get_args(argv)\n",
    "config = get_config(args)\n",
    "args.distributed = False\n",
    "print(config.dataset.train)\n",
    "config.dataset.train.others.bs = 1\n",
    "config.dataset.test.others.bs = 1\n",
    "config.dataset.test.others.model = config.model\n",
    "config.dataset.train.others.model = config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, loader = builder.dataset_builder(args, config.dataset.train)\n",
    "trainset = loader.dataset\n",
    "_, loader = builder.dataset_builder(args, config.dataset.test)\n",
    "testset = loader.dataset\n",
    "testset.normalization=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.transforms = transforms.Compose([])\n",
    "trainset.token_transforms = transforms.Compose([])\n",
    "trainset.normalization = True\n",
    "testset.normalization = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = np.empty((len(trainset), 3))\n",
    "y = np.empty(len(trainset))\n",
    "print(y.shape)\n",
    "for i, (neighborhood, center, target) in enumerate(trainset):\n",
    "    points, target = center.numpy(), target.numpy()\n",
    "    ptcloud = neighborhood.reshape(-1, 3).T\n",
    "    x, ys, z = ptcloud.numpy()\n",
    "    feat[i, 0:3]  = np.ptp(x), np.ptp(ys) , np.ptp(z)\n",
    "    y[i] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(feat, y)\n",
    "reg.score(feat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feattest = np.empty((len(testset), 3))\n",
    "ytest = np.empty(len(testset))\n",
    "for i, (neighborhood, center, target) in enumerate(testset):\n",
    "    points, target = center.numpy(), target.numpy()\n",
    "    ptcloud = neighborhood.reshape(-1, 3).T\n",
    "    x, ys, z = ptcloud.numpy()\n",
    "    feattest[i, 0:3] = np.ptp(x), np.ptp(ys) , np.ptp(z)\n",
    "    ytest[i] = target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = reg.predict(feattest)\n",
    "np.mean((prediction -ytest)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale up predictions to original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.load(\"experiments/regression/biomass_treeset/onlycenterfps/bestresultsforregression/predictions.npy\").reshape(-1)\n",
    "print(loss(torch.tensor(predictions), torch.tensor(ytest)))\n",
    "\n",
    "res = (predictions - ytest)**2\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(res) /len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ytest, predictions)\n",
    "plt.xlabel(\"normalized log tree volume\")\n",
    "plt.xlim(-2, 1.7)\n",
    "plt.ylim(-2, 1.7)\n",
    "plt.ylabel(\"predicted  normalized log tree volume\")\n",
    "x1, y1 = [-2, 1.7], [-2, 1.7]\n",
    "plt.plot(x1, y1, \"--\")\n",
    "plt.savefig(fname=\"custom/plot/log_predvsactual.svg\",format=\"svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os   \n",
    "import open3d as o3d\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass = np.load(\"../data/biomass/biomass.npy\")\n",
    "biomass = np.log(biomass)\n",
    "mean = biomass.mean()\n",
    "std = biomass.std()\n",
    "#biomass = (biomass - biomass.mean()) / biomass.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.exp(predictions * std_logmass + mean_logmass)\n",
    "#rescale predictions and values to original scale\n",
    "target = np.exp(ytest * std_logmass + mean_logmass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter((target), (pred))\n",
    "plt.xlabel(\"tree volume\")\n",
    "plt.ylabel(\"predicted tree volume\")\n",
    "plt.xlim(-1, 20)\n",
    "plt.ylim(-1, 20)\n",
    "x1, y1 = [-1, 20], [-1, 20]\n",
    "plt.plot(x1, y1, \"--\")\n",
    "plt.savefig(fname=\"custom/plot/predvsactual.svg\",format=\"svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(torch.tensor(pred), torch.tensor(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(res/ target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(res / target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00161b65f4bc80da0780f3897ff3169c7001edac6346be02133c1aa8f4f896a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
