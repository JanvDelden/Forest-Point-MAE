{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from davit import DaViT, CrossAttention\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = CrossAttention(\n",
    "            dim=384,\n",
    "            num_heads=3,\n",
    "            qkv_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, N, K = 12, 64, 7\n",
    "x_query = torch.ones(B*N, 1, 384)\n",
    "x_keyvalue = torch.ones(B*N, 7, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 64, 384])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(x_query, x_keyvalue).reshape(B, N, 384).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4128768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768*2*3*128*7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = DaViT(\n",
    "    embed_dims=(384, 384, 384, 384),\n",
    "    depths=(1, 1, 3, 1),\n",
    "    k=7,\n",
    "    drop_path_rate=0.1,\n",
    "    num_heads=(3, 6, 12, 24)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 16\n",
    "N = 64\n",
    "C = 384\n",
    "K = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(B, N, C)\n",
    "pos = torch.ones(B, N, C)\n",
    "neighbor_idx = torch.ones(B, N, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block done\n",
      "block done\n",
      "block done\n",
      "block done\n",
      "block done\n",
      "block done\n",
      "block done\n",
      "block done\n",
      "block done\n",
      "block done\n",
      "block done\n",
      "block done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 384])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks(x, pos, neighbor_idx.long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/user/jschnei2/miniconda3/envs/pointmae/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/srv/user/jschnei2/backup/models/davit.py\u001b[0m(13)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m                \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m                \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/user/jschnei2/miniconda3/envs/pointmae/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/srv/user/jschnei2/backup/models/davit.py\u001b[0m(300)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    298 \u001b[0;31m                    \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbranch_id\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    299 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 300 \u001b[0;31m                \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_blocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    301 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    302 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]])\n",
      "MySequential(\n",
      "  (0): LocalBlock(\n",
      "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): CrossAttention(\n",
      "      (q): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (kv): Linear(in_features=384, out_features=768, bias=True)\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ChannelBlock(\n",
      "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): ChannelAttention(\n",
      "      (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    )\n",
      "    (drop_path): DropPath()\n",
      "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "MySequential(\n",
      "  (0): LocalBlock(\n",
      "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): CrossAttention(\n",
      "      (q): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (kv): Linear(in_features=384, out_features=768, bias=True)\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    )\n",
      "    (drop_path): Identity()\n",
      "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (1): ChannelBlock(\n",
      "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): ChannelAttention(\n",
      "      (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    )\n",
      "    (drop_path): DropPath()\n",
      "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): Mlp(\n",
      "      (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (act): GELU()\n",
      "      (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "*** TypeError: forward() missing 1 required positional argument: 'neighbor_idx'\n",
      "*** TypeError: forward() takes 3 positional arguments but 4 were given\n",
      "*** TypeError: forward() takes 3 positional arguments but 4 were given\n",
      "> \u001b[0;32m/user/jschnei2/miniconda3/envs/pointmae/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/srv/user/jschnei2/backup/models/davit.py\u001b[0m(13)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m                \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m                \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "torch.Size([16, 64, 384])\n",
      "> \u001b[0;32m/user/jschnei2/miniconda3/envs/pointmae/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m(1102)\u001b[0;36m_call_impl\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1100 \u001b[0;31m        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
      "\u001b[0m\u001b[0;32m   1101 \u001b[0;31m                or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "\u001b[0m\u001b[0;32m-> 1102 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1103 \u001b[0;31m        \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1104 \u001b[0;31m        \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** Newest frame\n"
     ]
    }
   ],
   "source": [
    "%debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_idx=torch.ones(B, N, K).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(B, N, 384).cuda()\n",
    "batchnum = torch.repeat_interleave(torch.arange(B), N*K).to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 64, 384])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngb = torch.stack((batchnum, neighbor_idx.view(-1).cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5376])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 11, 11, 11],\n",
       "        [ 1,  1,  1,  ...,  1,  1,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngb.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 64, 384])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5376, 384])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[ngb.long().tolist()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[[0,0],[0,0]]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114688"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*128*7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pointmae': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c8a4ccd7c8b9a3d4af7060c69cfa16b451ab85bba45e86b03df6f484ab946ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
